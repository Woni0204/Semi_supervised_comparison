{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68bb18e1",
   "metadata": {},
   "source": [
    "# 데이터 스플릿해주는 코드는 나중에 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f9e5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb046ece",
   "metadata": {},
   "source": [
    "# 파이프라인 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2b271229-3629-424a-9e5c-9dcc531af0a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-15T14:19:31.179177Z",
     "iopub.status.busy": "2024-08-15T14:19:31.179093Z",
     "iopub.status.idle": "2024-08-15T14:19:31.374331Z",
     "shell.execute_reply": "2024-08-15T14:19:31.374062Z",
     "shell.execute_reply.started": "2024-08-15T14:19:31.179168Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 라이브러리 및 모듈을 불러옴.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# matplotlib의 기본 이미지 컬러맵을 'gray'로 설정.\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# 이미지 처리를 위한 OpenCV와 PIL 라이브러리를 불러옴.\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# 난수 생성을 위한 random 모듈과 객체 복사를 위한 copy 모듈을 불러옴.\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# 경고 메시지를 무시하도록 설정.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 현재 세션의 파일 이름을 가져와 FILENAME 변수에 저장.\n",
    "FILENAME = os.getcwd()+'/'+str(__session__).split('/')[-1]\n",
    "\n",
    "# PyTorch 관련 모듈을 불러옴.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# TorchVision을 사용한 데이터셋과 전처리 도구들을 불러옴.\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import RandomResizedCrop\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# 데이터셋 및 데이터 로더를 위한 유틸리티를 불러옴.\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "# 데이터 분할을 위한 Scikit-Learn의 유틸리티를 불러옴.\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# MONAI 라이브러리에서 손실 함수와 메트릭을 불러옴.\n",
    "from monai.losses import TverskyLoss as TverskyLoss\n",
    "from monai.transforms import Compose, ToTensor, RandFlip\n",
    "from monai.metrics import DiceMetric as Dice_Function\n",
    "from monai.metrics import compute_iou as IoU_Function\n",
    "from monai.metrics import ConfusionMatrixMetric\n",
    "\n",
    "# 시스템 모듈의 경로에 상위 디렉토리를 추가.\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# 모델이 위치한 디렉토리를 지정.\n",
    "model_dir = 'models'\n",
    "\n",
    "# 모델 디렉토리에서 사용할 모듈 이름을 리스트로 지정. (ST, ST++, AugSeg, S4Former)\n",
    "module_names = ['ST', 'ST++', 'AugSeg']\n",
    "model_names = module_names\n",
    "\n",
    "# 지정된 모듈을 동적으로 불러옴.\n",
    "for module_name in module_names:\n",
    "    exec(f'from {model_dir}.{module_name} import *')\n",
    "\n",
    "# 반복 횟수를 지정. (추후 데이터 스플릿한 폴더 지정하여 반복실험 가능하게끔)\n",
    "iterations = [1, 10]\n",
    "\n",
    "# 입력 채널 수와 클래스 수를 설정.\n",
    "in_channels = 3\n",
    "number_of_classes = 1\n",
    "\n",
    "# 학습에 사용할 에포크 수를 설정.\n",
    "epochs = 80  # PASCAL VOC 2012 기준: 80 epochs\n",
    "\n",
    "# 얼리 스탑핑을 위한 기준 에포크 수를 설정.\n",
    "EARLY_STOP = 25  \n",
    "\n",
    "# 배치 크기를 설정.\n",
    "batch_size = 16\n",
    "\n",
    "# 사용할 GPU 장치 번호를 설정. (GALAX RTX 3090 EX GAMER)\n",
    "devices = [0]\n",
    "\n",
    "# 옵티마이저의 종류와 학습률, 모멘텀, 가중치 감소율을 설정.\n",
    "backbone_lr = 1e-3\n",
    "segmentation_head_lr = 1e-2  # 10배 큰 학습률\n",
    "weight_decay = 1e-4\n",
    "optim_args = {'backbone_lr': backbone_lr, 'segmentation_head_lr': segmentation_head_lr, 'weight_decay': weight_decay}\n",
    "\n",
    "# 학습률 스케줄러로 Poly LR을 설정하고 관련 매개변수를 지정.\n",
    "def poly_lr_scheduler(optimizer, init_lr, iter, max_iter, power=0.9):\n",
    "    new_lr = init_lr * (1 - iter / max_iter) ** power\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "    return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "28c38740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수로 Cross-Entropy Loss와 Consistency Regularization Loss를 포함하는 손실 함수를 설정\n",
    "class CrossEntropyWithConsistencyLoss(nn.Module):\n",
    "    def __init__(self, lambda_consistency=1.0):\n",
    "        super(CrossEntropyWithConsistencyLoss, self).__init__()\n",
    "        self.lambda_consistency = lambda_consistency\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, student_outputs, teacher_outputs, targets):\n",
    "        ce_loss = self.ce_loss(student_outputs, targets)\n",
    "        consistency_loss = torch.mean((student_outputs - teacher_outputs) ** 2)\n",
    "        return ce_loss + self.lambda_consistency * consistency_loss\n",
    "\n",
    "# 랜덤 시드를 설정하는 함수.\n",
    "def control_random_seed(seed, pytorch=True):\n",
    "    random.seed(seed)  # Python의 기본 random 모듈 시드 설정\n",
    "    np.random.seed(seed)  # NumPy의 시드 설정\n",
    "    try:\n",
    "        torch.manual_seed(seed)  # PyTorch의 시드 설정\n",
    "        if torch.cuda.is_available() == True:  # CUDA 사용 가능 여부 확인\n",
    "            torch.cuda.manual_seed(seed)  # GPU의 시드 설정\n",
    "            torch.cuda.manual_seed_all(seed)  # 모든 GPU에 대해 시드 설정\n",
    "            torch.backends.cudnn.deterministic = True  # CUDA 연산을 결정론적으로 설정\n",
    "            torch.backends.cudnn.benchmark = False  # 연산 속도 최적화를 비활성화\n",
    "    except:\n",
    "        pass\n",
    "        torch.backends.cudnn.benchmark = False  # 예외 발생 시 연산 속도 최적화를 비활성화\n",
    "\n",
    "# 파일 경로에서 이미지를 읽어오는 함수.\n",
    "def imread_kor(filePath, mode=cv2.IMREAD_UNCHANGED):\n",
    "    stream = open(filePath.encode(\"utf-8\"), \"rb\")  # 파일을 바이너리 읽기 모드로 열기\n",
    "    bytes = bytearray(stream.read())  # 파일 데이터를 바이트 배열로 읽기\n",
    "    numpyArray = np.asarray(bytes, dtype=np.uint8)  # 바이트 배열을 NumPy 배열로 변환\n",
    "    return cv2.imdecode(numpyArray, mode)  # NumPy 배열을 이미지로 디코딩하여 반환\n",
    "\n",
    "# 파일 경로에 이미지를 저장하는 함수.\n",
    "def imwrite_kor(filename, img, params=None):\n",
    "    try:\n",
    "        ext = os.path.splitext(filename)[1]  # 파일 확장자 추출\n",
    "        result, n = cv2.imencode(ext, img, params)  # 이미지를 인코딩하여 바이트 배열로 변환\n",
    "        if result:\n",
    "            with open(filename, mode='w+b') as f:  # 바이너리 쓰기 모드로 파일 열기\n",
    "                n.tofile(f)  # 바이트 배열을 파일에 쓰기\n",
    "                return True\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(e)  # 예외 발생 시 오류 메시지 출력\n",
    "        return False\n",
    "\n",
    "# 이미지와 마스크를 주어진 각도 범위 내에서 무작위로 회전시키는 함수.\n",
    "def random_rotation(image, mask, angle_range=(-30, 30)):\n",
    "    angle = random.uniform(angle_range[0], angle_range[1])  # 주어진 범위에서 무작위 각도 선택\n",
    "    image = TF.rotate(image, angle)  # 이미지 회전\n",
    "    mask = TF.rotate(mask, angle)  # 마스크 회전\n",
    "    return image, mask\n",
    "\n",
    "# 이미지와 마스크 경로 리스트를 받아 데이터셋을 만드는 클래스.\n",
    "class ImagesDataset(Dataset):\n",
    "    def __init__(self, image_path_list, target_path_list, aug=False):\n",
    "        self.image_path_list = image_path_list  # 이미지 경로 리스트\n",
    "        self.target_path_list = target_path_list  # 마스크 경로 리스트\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),  # 이미지를 텐서로 변환\n",
    "        ])\n",
    "        self.aug = aug  # 데이터 증강 여부 설정\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path_list)  # 데이터셋의 길이 반환\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_path_list[idx]  # 인덱스에 해당하는 이미지 경로 가져오기\n",
    "        mask_path = self.target_path_list[idx]  # 인덱스에 해당하는 마스크 경로 가져오기\n",
    "        image = imread_kor(image_path)  # 이미지 읽기\n",
    "        image = self.transform(image).float()  # 이미지 변환 및 float형 변환\n",
    "\n",
    "        mask = imread_kor(mask_path)  # 마스크 읽기\n",
    "        mask = self.transform(mask).float()  # 마스크 변환 및 float형 변환\n",
    "\n",
    "        # 데이터 증강이 활성화된 경우, 추가적인 변환을 수행.\n",
    "        if self.aug == True:\n",
    "            if random.random() < 0.5:\n",
    "                resize_transform = RandomResizedCrop(size=(384, 256))  # 무작위 크롭 및 리사이즈\n",
    "                i, j, h, w = resize_transform.get_params(image, scale=(0.7, 1.0), ratio=(1, 1))\n",
    "                image = TF.resized_crop(image, i, j, h, w, (384, 256))\n",
    "            if random.random() < 0.5:\n",
    "                image = RandFlip(1, 0)(image)  # 좌우 반전\n",
    "                mask = RandFlip(1, 0)(mask)  # 좌우 반전\n",
    "            if random.random() < 0.5:\n",
    "                image, mask = random_rotation(image, mask)  # 무작위 회전\n",
    "\n",
    "        mask[mask > 0] = 1  # 마스크 값을 이진화\n",
    "        return image, mask, image_path  # 이미지, 마스크, 이미지 경로 반환\n",
    "\n",
    "# 픽셀 정확도를 계산하는 함수.\n",
    "def Pixel_Accuracy(yhat, ytrue, threshold=0.5):\n",
    "    yhat = yhat > threshold  # 예측값을 이진화\n",
    "    correct = torch.sum(yhat == ytrue)  # 예측이 실제값과 일치하는 픽셀 수 계산\n",
    "    total = ytrue.numel()  # 총 픽셀 수 계산\n",
    "    accuracy = correct.float() / total  # 정확도 계산\n",
    "    return accuracy.item()\n",
    "\n",
    "# Intersection over Union (IoU)를 계산하는 함수.\n",
    "def Intersection_over_Union(yhat, ytrue, threshold=0.5):\n",
    "    yhat = yhat > threshold  # 예측값을 이진화\n",
    "    return IoU_Function(yhat, ytrue).nanmean().item()  # IoU 계산 및 반환\n",
    "\n",
    "# Dice 계수를 계산하는 함수.\n",
    "def Dice_Coefficient(yhat, ytrue, threshold=0.5):\n",
    "    yhat = yhat > threshold  # 예측값을 이진화\n",
    "    return Dice_Function()(yhat, ytrue).nanmean().item()  # Dice 계수 계산 및 반환\n",
    "\n",
    "# 혼동 행렬을 계산하는 함수.\n",
    "def Confusion_Matrix(yhat, ytrue, threshold=0.5):\n",
    "    yhat = yhat > threshold  # 예측값을 이진화\n",
    "    confusion_matrix = ConfusionMatrixMetric(metric_name=[\"recall\", \"precision\", \"f1 score\"], reduction='mean', compute_sample=True)\n",
    "    confusion_matrix(yhat, ytrue)  # 혼동 행렬 계산\n",
    "    recall, precision, f1 = confusion_matrix.aggregate()  # 재현율, 정밀도, F1 점수 계산\n",
    "    return recall, precision, f1\n",
    "\n",
    "# 모델 학습을 위한 함수.\n",
    "def train(train_loader, epoch, model, criterion, optimizer, device, max_iter):\n",
    "    model.train()  # 모델을 학습 모드로 전환\n",
    "    train_losses = AverageMeter()  # 손실 값을 저장할 객체 생성\n",
    "    for i, (input, target, _) in enumerate(train_loader):\n",
    "        iter_num = epoch * len(train_loader) + i\n",
    "        poly_lr_scheduler(optimizer, backbone_lr, iter_num, max_iter)  # Poly learning rate scheduling 적용\n",
    "        \n",
    "        input = input.to(device)  # 입력을 GPU로 전송\n",
    "        target = target.to(device)  # 타겟을 GPU로 전송\n",
    "        output = model(input)  # 모델에 입력을 전달\n",
    "        teacher_output = model(input).detach()  # 교사 모델의 출력을 얻음\n",
    "        loss = criterion(output, teacher_output, target).float()  # 손실 계산\n",
    "        optimizer.zero_grad()  # 옵티마이저의 기울기 초기화\n",
    "        loss.backward()  # 역전파를 통해 기울기 계산\n",
    "        optimizer.step()  # 옵티마이저를 통해 가중치 갱신\n",
    "        train_losses.update(loss.detach().cpu().numpy(), input.shape[0])  # 손실 값을 업데이트\n",
    "    Train_Loss = np.round(train_losses.avg, 6)  # 평균 손실 값을 반올림\n",
    "    return Train_Loss\n",
    "\n",
    "# 모델 검증을 위한 함수.\n",
    "def validate(validation_loader, model, criterion, device, model_path=False, return_image_paths=False):\n",
    "    if model_path != False:\n",
    "        model.load_state_dict(torch.load(model_path))  # 모델 가중치를 로드\n",
    "    model.eval()  # 모델을 평가 모드로 전환\n",
    "    for i, (input, target, image_path) in enumerate(validation_loader):\n",
    "        input = input.to(device)  # 입력을 GPU로 전송\n",
    "        target = target.to(device)  # 타겟을 GPU로 전송\n",
    "        with torch.no_grad():  # 기울기 계산 비활성화\n",
    "            output = model(input)  # 모델에 입력을 전달\n",
    "            teacher_output = model(input).detach()  # 교사 모델의 출력을 얻음\n",
    "            loss = criterion(output, teacher_output, target).float()  # 손실 계산\n",
    "        if i == 0:\n",
    "            targets = target  # 첫 번째 배치의 타겟 저장\n",
    "            outputs = output  # 첫 번째 배치의 출력 저장\n",
    "            if return_image_paths == True:\n",
    "                image_paths = image_path  # 이미지 경로 저장\n",
    "        else:\n",
    "            targets = torch.cat((targets, target))  # 이후 배치의 타겟을 연결\n",
    "            outputs = torch.cat((outputs, output), axis=0)  # 이후 배치의 출력을 연결\n",
    "            if return_image_paths == True:\n",
    "                image_paths += image_path  # 이미지 경로를 연결\n",
    "    if return_image_paths == True:\n",
    "        return outputs, targets, image_paths  # 출력, 타겟, 이미지 경로 반환\n",
    "    return outputs, targets  # 출력과 타겟 반환\n",
    "\n",
    "# 클래스 이름을 문자열로 받아 해당 클래스 객체를 반환하는 함수.\n",
    "def str_to_class(classname):\n",
    "    return getattr(sys.modules[__name__], classname)\n",
    "\n",
    "# 소스 파일을 출력 디렉토리로 복사하는 함수.\n",
    "def copy_sourcefile(output_dir, src_dir='src'):\n",
    "    import os\n",
    "    import shutil\n",
    "    import glob\n",
    "    source_dir = os.path.join(output_dir, src_dir)\n",
    "\n",
    "    os.makedirs(source_dir, exist_ok=True)  # 소스 디렉토리 생성\n",
    "    org_files1 = os.path.join('./', '*.py')  # .py 파일 경로 설정\n",
    "    org_files2 = os.path.join('./', '*.sh')  # .sh 파일 경로 설정\n",
    "    org_files3 = os.path.join('./', '*.ipynb')  # .ipynb 파일 경로 설정\n",
    "    org_files4 = os.path.join('./', '*.txt')  # .txt 파일 경로 설정\n",
    "    org_files5 = os.path.join('./', '*.json')  # .json 파일 경로 설정\n",
    "    files = []\n",
    "    files = glob.glob(org_files1)\n",
    "    files += glob.glob(org_files2)\n",
    "    files += glob.glob(org_files3)\n",
    "    files += glob.glob(org_files4)\n",
    "    files += glob.glob(org_files5)\n",
    "\n",
    "    # 소스 파일들을 출력 디렉토리로 복사\n",
    "    tgt_files = os.path.join(source_dir, '.')\n",
    "    for i, file in enumerate(files):\n",
    "        shutil.copy(file, tgt_files)\n",
    "\n",
    "# 학습 및 검증 손실 값을 저장하고 관리하는 클래스.\n",
    "class LossSaver(object):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []  # 학습 손실 값 저장 리스트\n",
    "        self.val_losses = []  # 검증 손실 값 저장 리스트\n",
    "\n",
    "    def reset(self):\n",
    "        self.train_losses = []  # 학습 손실 값 초기화\n",
    "        self.val_losses = []  # 검증 손실 값 초기화\n",
    "\n",
    "    def update(self, train_loss, val_loss):\n",
    "        self.train_losses.append(train_loss)  # 학습 손실 값 업데이트\n",
    "        self.val_losses.append(val_loss)  # 검증 손실 값 업데이트\n",
    "\n",
    "    def return_list(self):\n",
    "        return self.train_losses, self.val_losses  # 학습 및 검증 손실 값 리스트 반환\n",
    "\n",
    "    def save_as_csv(self, csv_file):\n",
    "        df = pd.DataFrame({'Train Losses': self.train_losses, 'Validation Losses': self.val_losses})  # 손실 값들을 데이터프레임으로 저장\n",
    "        df.index = [f\"{i + 1} Epoch\" for i in df.index]  # 인덱스를 에포크로 설정\n",
    "        df.to_csv(csv_file, index=True)  # CSV 파일로 저장\n",
    "\n",
    "# 평균 값을 계산하고 업데이트하는 클래스.\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0  # 현재 값 초기화\n",
    "        self.avg = 0  # 평균 값 초기화\n",
    "        self.sum = 0  # 합계 초기화\n",
    "        self.count = 0  # 카운트 초기화\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val  # 현재 값 업데이트\n",
    "        self.sum += val * n  # 합계 업데이트\n",
    "        self.count += n  # 카운트 업데이트\n",
    "        self.avg = self.sum / self.count  # 평균 값 계산\n",
    "\n",
    "# 이미지 경로와 마스크 경로를 수집하는 함수.\n",
    "def collect_image_paths(base_dataset_dir, sub_dirs=['training', 'validation', 'test']):\n",
    "    train_image_path_list = []  # 학습 이미지 경로 리스트\n",
    "    train_target_path_list = []  # 학습 마스크 경로 리스트\n",
    "    validation_image_path_list = []  # 검증 이미지 경로 리스트\n",
    "    validation_target_path_list = []  # 검증 마스크 경로 리스트\n",
    "    test_image_path_list = []  # 테스트 이미지 경로 리스트\n",
    "    test_target_path_list = []  # 테스트 마스크 경로 리스트\n",
    "\n",
    "    # 서브 디렉토리 이름과 리스트 매핑\n",
    "    dir_to_lists = {\n",
    "        'training': (train_image_path_list, train_target_path_list),\n",
    "        'validation': (validation_image_path_list, validation_target_path_list),\n",
    "        'test': (test_image_path_list, test_target_path_list)\n",
    "    }\n",
    "\n",
    "    for sub_dir in sub_dirs:\n",
    "        full_dir_path = os.path.join(base_dataset_dir, sub_dir)  # 각 서브 디렉토리의 전체 경로를 생성\n",
    "\n",
    "        image_list, target_list = dir_to_lists.get(sub_dir, (None, None))  # 해당 서브 디렉토리에 대한 이미지 및 마스크 리스트 선택\n",
    "\n",
    "        if image_list is None or target_list is None:\n",
    "            print(f\"Unknown sub-directory: {sub_dir}\")  # 알 수 없는 서브 디렉토리일 경우 경고 출력\n",
    "            continue\n",
    "\n",
    "        for file_name in os.listdir(full_dir_path):  # 서브 디렉토리 내의 파일들을 검색\n",
    "            if (file_name.endswith(\".png\") or file_name.endswith(\".jpg\")) and \"_mask\" not in file_name:\n",
    "                image_list.append(os.path.join(full_dir_path, file_name))  # 이미지 파일 경로 추가\n",
    "\n",
    "                if file_name.endswith(\".png\"):\n",
    "                    mask_file_name = file_name.replace(\".png\", \"_mask.png\")  # PNG 파일에 대응하는 마스크 이름 처리\n",
    "                elif file_name.endswith(\".jpg\"):\n",
    "                    mask_file_name = file_name.replace(\".jpg\", \"_mask.jpg\")  # JPG 파일에 대응하는 마스크 이름 처리\n",
    "\n",
    "                target_list.append(os.path.join(full_dir_path, mask_file_name))  # 마스크 파일 경로 추가\n",
    "\n",
    "    return (train_image_path_list, train_target_path_list,\n",
    "            validation_image_path_list, validation_target_path_list,\n",
    "            test_image_path_list, test_target_path_list)\n",
    "\n",
    "# 실험을 수행하는 함수.\n",
    "def Do_Experiment(iteration, model_name, model, train_loader, validation_loader, test_loader, Optimizer, lr, number_of_classes, epochs, Metrics, df, device, transform):\n",
    "    start = timeit.default_timer()  # 실험 시작 시간을 기록\n",
    "    train_bool = True\n",
    "    test_bool = True\n",
    "    \n",
    "    criterion = CrossEntropyWithConsistencyLoss()  # Cross-Entropy와 Consistency Regularization을 포함한 손실 함수 설정\n",
    "    \n",
    "    # backbone과 segmentation head의 학습률을 다르게 적용하는 optimizer 설정\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': model.backbone.parameters(), 'lr': backbone_lr},\n",
    "        {'params': model.segmentation_head.parameters(), 'lr': segmentation_head_lr}\n",
    "    ], weight_decay=weight_decay)\n",
    "    \n",
    "    max_iter = epochs * len(train_loader)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)  # 출력 디렉토리 생성\n",
    "    control_random_seed(seed)  # 랜덤 시드 설정\n",
    "    if train_bool:\n",
    "        now = datetime.now()\n",
    "        Train_date = now.strftime(\"%y%m%d_%H%M%S\")  # 현재 시간 기록\n",
    "        print('Training Start Time:', Train_date)\n",
    "        best = 9999\n",
    "        best_epoch = 1\n",
    "        Early_Stop = 0\n",
    "        loss_saver = LossSaver()  # 손실 값을 저장할 객체 생성\n",
    "        train_start_time = timeit.default_timer()\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            Train_Loss = train(train_loader, epoch, model, criterion, optimizer, device, max_iter)  # 한 에포크 동안 학습 수행\n",
    "            outputs, targets = validate(validation_loader, model, criterion, device)  # 검증 수행\n",
    "            Val_Loss = np.round(criterion(outputs, targets).cpu().numpy(), 6)  # 검증 손실 값 계산\n",
    "            iou = np.round(Intersection_over_Union(outputs, targets), 3)  # IoU 계산\n",
    "            dice = np.round(Dice_Coefficient(outputs, targets), 3)  # Dice 계수 계산\n",
    "            now = datetime.now()\n",
    "            date = now.strftime(\"%y%m%d_%H%M%S\")\n",
    "            print(str(epoch) + 'EP(' + date + '):', end=' ')\n",
    "            print('T_Loss: ' + str(Train_Loss), end=' ')\n",
    "            print('V_Loss: ' + str(Val_Loss), end=' ')\n",
    "            print('IoU: ' + str(iou), end=' ')\n",
    "            print('Dice: ' + str(dice), end=' ')\n",
    "\n",
    "            loss_saver.update(Train_Loss, Val_Loss)  # 손실 값 업데이트\n",
    "            loss_saver.save_as_csv(f'{output_dir}/Losses_{Experiments_Time}.csv')  # 손실 값을 CSV로 저장\n",
    "            if Val_Loss < best:\n",
    "                Early_Stop = 0\n",
    "                torch.save(model.state_dict(), f'{output_dir}/{Train_date}_{model_name}_Iter_{iteration}.pt')  # 모델 가중치 저장\n",
    "                best_epoch = epoch  # 베스트 에포크 업데이트\n",
    "                best = Val_Loss  # 베스트 손실 값 업데이트\n",
    "                print('Best Epoch:', best_epoch, 'Loss:', Val_Loss)\n",
    "            else:\n",
    "                print('')\n",
    "                Early_Stop += 1\n",
    "            if Early_Stop >= EARLY_STOP:  # 얼리 스탑 조건 확인\n",
    "                break\n",
    "        train_stop_time = timeit.default_timer()  # 학습 종료 시간 기록\n",
    "    if test_bool:\n",
    "        now = datetime.now()\n",
    "        date = now.strftime(\"%y%m%d_%H%M%S\")\n",
    "        print('Test Start Time:', date)\n",
    "        outputs, targets, image_paths = validate(test_loader, model, criterion, device,\n",
    "                                                 model_path=f'{output_dir}/{Train_date}_{model_name}_Iter_{iteration}.pt',\n",
    "                                                 return_image_paths=True)  # 테스트 수행\n",
    "        Loss = np.round(criterion(outputs, targets).cpu().numpy(), 6)  # 테스트 손실 값 계산\n",
    "        pa = np.round(Pixel_Accuracy(outputs.cpu(), targets.cpu()), 3)  # 픽셀 정확도 계산\n",
    "        iou = np.round(Intersection_over_Union(outputs, targets), 3)  # IoU 계산\n",
    "        dice = np.round(Dice_Coefficient(outputs, targets), 3)  # Dice 계수 계산\n",
    "        recall, precision, f1 = Confusion_Matrix(outputs, targets)  # 혼동 행렬 계산\n",
    "        recall = np.round(recall.cpu().numpy()[0], 3); precision = np.round(precision.cpu().numpy()[0], 3); f1 = np.round(f1.cpu().numpy()[0], 3);\n",
    "\n",
    "        now = datetime.now()\n",
    "        date = now.strftime(\"%y%m%d_%H%M%S\")\n",
    "        print('Best Epoch:', best_epoch)\n",
    "        print('Test(' + date + '): ' + 'Loss: ' + str(Loss), end=' ')\n",
    "        print('PA: ' + str(pa), end=' ')\n",
    "        print('IoU: ' + str(iou), end=' ')\n",
    "        print('Dice: ' + str(dice), end=' ')\n",
    "        print('Recall: ' + str(recall), end=' ')\n",
    "        print('Precision: ' + str(precision), end=' ')\n",
    "        print('F1 Score: ' + str(f1), end='\\n')\n",
    "\n",
    "        stop = timeit.default_timer(); m, s = divmod((train_stop_time - train_start_time) / epoch, 60); h, m = divmod(m, 60); Time_per_Epoch = \"%02d:%02d:%02d\" % (h, m, s);\n",
    "        m, s = divmod(stop - start, 60); h, m = divmod(m, 60); Time = \"%02d:%02d:%02d\" % (h, m, s);\n",
    "        total_params = sum(p.numel() for p in model.parameters()); total_params = format(total_params, ',');\n",
    "        Performances = [Experiments_Time, Train_date, iteration, model_name, best, Loss, pa, iou, dice, recall, precision, f1, total_params, Time, best_epoch, Time_per_Epoch, loss_function, lr, batch_size, epochs, FILENAME]\n",
    "        df = df.append(pd.Series(Performances, index=df.columns), ignore_index=True)  # 실험 성능을 데이터프레임에 추가\n",
    "        os.makedirs(f'{output_dir}/test_outputs', exist_ok=True)  # 출력 디렉토리 생성\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        for output, image_path in zip(outputs, image_paths):\n",
    "            np.save(f'{output_dir}/test_outputs/{os.path.basename(image_path)}', output)  # 출력 저장\n",
    "    now = datetime.now()\n",
    "    date = now.strftime(\"%y%m%d_%H%M%S\")\n",
    "    print('End', date)\n",
    "\n",
    "    return df  # 데이터프레임 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "96ec659d-2ef6-4323-8dc2-7a0105a5732d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-15T14:19:31.374928Z",
     "iopub.status.busy": "2024-08-15T14:19:31.374781Z",
     "iopub.status.idle": "2024-08-15T14:19:31.393918Z",
     "shell.execute_reply": "2024-08-15T14:19:31.393723Z",
     "shell.execute_reply.started": "2024-08-15T14:19:31.374919Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 현재 시간을 기록하고, 실험 시작 시간을 문자열로 변환하여 저장.\n",
    "now = datetime.now()\n",
    "Experiments_Time = now.strftime(\"%y%m%d_%H%M%S\")\n",
    "print('Experiment Start Time:', Experiments_Time)  # 실험 시작 시간을 출력.\n",
    "\n",
    "# 실험 성능을 저장할 데이터프레임의 열을 정의.\n",
    "Metrics = ['Experiment Time', 'Train Time', 'Iteration', 'Model Name', 'Val_Loss', \n",
    "           'Test_Loss', 'PA', 'IoU', 'Dice', 'Recall', 'Precision', 'F1 Score', \n",
    "           'Total Params', 'Train-Prediction Time', 'Best Epoch', 'Time per Epoch', \n",
    "           'Loss Function', 'LR', 'Batch size', '#Epochs', 'DIR']\n",
    "df = pd.DataFrame(index=None, columns=Metrics)  # 빈 데이터프레임을 생성.\n",
    "\n",
    "# 실험 결과를 저장할 디렉토리를 생성.\n",
    "output_root = f'output/output_{Experiments_Time}'\n",
    "os.makedirs(output_root, exist_ok=True)  # 출력 디렉토리를 생성.\n",
    "\n",
    "# 설정된 반복 횟수만큼 반복.\n",
    "for iteration in range(iterations[0], iterations[1] + 1):\n",
    "    seed = iteration  # 현재 반복의 시드를 설정.\n",
    "    control_random_seed(seed)  # 랜덤 시드를 설정.\n",
    "\n",
    "    # 각 반복(iteration)에 맞는 데이터셋 분할 폴더를 설정.\n",
    "    Dataset_dir = f'dataset/splits/pascal/1_8/split_{str(iteration).zfill(2)}'\n",
    "\n",
    "    # 이미지 경로와 마스크 경로를 수집.\n",
    "    (train_image_path_list, train_target_path_list,\n",
    "     validation_image_path_list, validation_target_path_list,\n",
    "     test_image_path_list, test_target_path_list) = collect_image_paths(Dataset_dir)\n",
    "\n",
    "    # 학습, 검증, 테스트 데이터셋을 생성.\n",
    "    train_dataset = ImagesDataset(train_image_path_list, train_target_path_list, aug=True)\n",
    "    validation_dataset = ImagesDataset(validation_image_path_list, validation_target_path_list, aug=False)\n",
    "    test_dataset = ImagesDataset(test_image_path_list, test_target_path_list, aug=False)\n",
    "    \n",
    "    # 데이터 로더를 생성.\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size,\n",
    "        num_workers=0, pin_memory=True, shuffle=True,\n",
    "    )\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset, batch_size=batch_size,\n",
    "        num_workers=0, pin_memory=True,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size,\n",
    "        num_workers=0, pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # 지정된 모델 목록에 대해 반복.\n",
    "    for model_name in model_names:\n",
    "        print(f'{model_name} (Iter {iteration})')  # 현재 모델과 반복(iteration) 번호를 출력.\n",
    "        output_dir = output_root + f'/{model_name}_Iter_{iteration}'  # 출력 디렉토리를 설정.\n",
    "        copy_sourcefile(output_dir, src_dir='src')  # 소스 파일을 출력 디렉토리로 복사.\n",
    "        control_random_seed(seed)  # 다시 랜덤 시드를 설정.\n",
    "        \n",
    "        # 모델 인스턴스를 생성.\n",
    "        model = str_to_class(model_name)(in_channels, number_of_classes)\n",
    "        device = torch.device(\"cuda:\" + str(devices[0]))  # 사용할 GPU 장치를 설정.\n",
    "        \n",
    "        # 여러 GPU를 사용할 경우, DataParallel로 모델을 래핑.\n",
    "        if len(devices) > 1:\n",
    "            model = torch.nn.DataParallel(model, device_ids=devices).to(device)\n",
    "        else:\n",
    "            model = model.to(device)  # 단일 GPU를 사용할 경우, 모델을 GPU로 전송.\n",
    "\n",
    "        # 실험을 수행하고, 결과를 데이터프레임에 저장.\n",
    "        df = Do_Experiment(seed, model_name, model, train_loader, validation_loader, test_loader, \n",
    "                           optimizer, lr, number_of_classes, epochs, Metrics, df, device, None)\n",
    "        \n",
    "        # 실험 결과를 CSV 파일로 저장.\n",
    "        try:\n",
    "            df.to_csv(output_root + '/' + 'Semi_Seg' + Experiments_Time + '.csv', \n",
    "                      index=False, header=True, encoding=\"cp949\")\n",
    "        except:\n",
    "            now = datetime.now()\n",
    "            tmp_date = now.strftime(\"%y%m%d_%H%M%S\")\n",
    "            df.to_csv(output_root + '/' + 'Semi_Seg' + Experiments_Time + '_' + tmp_date + '_tmp' + '.csv', \n",
    "                      index=False, header=True, encoding=\"cp949\")\n",
    "\n",
    "# 실험 종료를 알리고, 프로그램을 종료.\n",
    "import os\n",
    "print('End')\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44627737-1da2-473c-90f7-9ded7c16c400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
